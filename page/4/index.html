<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLM Security Group 's Notes - 分享知识，认识世界</title><meta name="author" content="LLM Security Group"><meta name="copyright" content="LLM Security Group"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="分享知识，认识世界">
<meta property="og:type" content="website">
<meta property="og:title" content="LLM Security Group &#39;s Notes">
<meta property="og:url" content="https://fdreamer2002.github.io/page/4/index.html">
<meta property="og:site_name" content="LLM Security Group &#39;s Notes">
<meta property="og:description" content="分享知识，认识世界">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://fdreamer2002.github.io/img/butterfly-icon.png">
<meta property="article:author" content="LLM Security Group">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://fdreamer2002.github.io/img/butterfly-icon.png"><script type="application/ld+json"></script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://fdreamer2002.github.io/page/4/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":"ture","top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM Security Group \'s Notes',
  isHighlightShrink: false,
  isToc: false,
  pageType: 'home'
}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/scroll.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/gradient.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lxgw-wenkai-screen-webfont@1.7.0/style.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fontsource/jetbrains-mono@4.5.12/index.min.css"><meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">77</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">81</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">28</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><span> 类别分类</span></a></li><li><a class="site-page child" href="/authors/"><span> 作者分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">LLM Security Group 's Notes</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><span> 类别分类</span></a></li><li><a class="site-page child" href="/authors/"><span> 作者分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="site-info"><h1 id="site-title">LLM Security Group 's Notes</h1></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts nc" id="recent-posts"><div class="recent-post-items"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/09/04/%E4%BC%8D%E4%BF%8A/2025-09-07/Exploring%20Multi-View%20Pixel%20Contrast%20for%20General%20and%20Robust%20Image%20Forgery%20Localization/" title="Exploring Multi-View Pixel Contrast for General and Robust Image Forgery Localization">Exploring Multi-View Pixel Contrast for General and Robust Image Forgery Localization</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-09-04T11:53:16.000Z" title="发表于 2025-09-04 19:53:16">2025-09-04</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E5%9B%BE%E5%83%8F%E4%BC%AA%E9%80%A0%E5%8F%96%E8%AF%81/">图像伪造取证</a></span></div><div class="content">英文题目：《Exploring Multi-View Pixel Contrast for General and Robust Image Forgery Localization》 中文题目：《探索多视角像素对比度以实现通用且稳健的图像伪造定位》 论文作者：Zijie Lou; Gang Cao; Kun Guo; Lifang Yu; Shaowei Weng 发布于：IEEE Transactions on Information Forensics and Security 发布时间：2025-02-13 级别：CCF-A 论文链接：10.1109/TIFS.2025.3541957 论文代码：https://github.com/multimediaFor/MPC  摘要 图像伪造定位旨在分割图像中的篡改区域，是一项基础而又极具挑战性的数字取证任务。虽然一些基于深度学习的取证方法取得了令人瞩目的成果，**但它们直接学习像素到标签的映射，而没有充分利用特征空间中像素之间的关系。**为了解决这一缺陷，我们提出了一种用于图像伪造定位的多视角逐像素对比算法 (MPC)。具体而...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/08/30/%E5%8F%A4%E4%B9%89%E7%BF%94/2025-08-30/PRP%20Propagating%20Universal%20Perturbations%20to%20Attack%20Large%20Language%20Model%20Guard-Rails/" title="PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails">PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-08-30T00:58:14.000Z" title="发表于 2025-08-30 08:58:14">2025-08-30</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%B6%8A%E7%8B%B1%E6%94%BB%E5%87%BB/">越狱攻击</a></span></div><div class="content">英文题目：《PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails》 中文题目：《PRP：传播通用扰动以攻击大型语言模型防护机制》 论文作者： Neal Mangaokar, Ashish Hooda, Jihye Choi, Shreyas Chandrashekaran, Kassem Fawaz, Somesh Jha, Atul Prakash 发布于： ACL 发布时间：2024-02-24 级别：CFF A 论文链接： https://arxiv.org/abs/2402.15911 论文代码：  摘要 大型语言模型（LLM）通常被设定为对人类无害。不幸的是，最近的研究表明，这类模型容易受到自动化越狱攻击，这些攻击会诱使它们生成有害内容。最新的LLM通常包含额外的防御层，即守卫模型，这是一个二级LLM，用于检查和调节主要LLM的输出响应。我们的主要贡献是提出了一种新颖的攻击策略PRP，该策略针对多个开源（例如Llama 2）和闭源（例如GPT 3.5）的守卫...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/08/29/%E6%98%93%E5%AD%90%E6%96%87/2025-08-30/Universal%20adversarial%20perturbations/" title="Universal adversarial perturbations">Universal adversarial perturbations</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-08-29T15:05:14.000Z" title="发表于 2025-08-29 23:05:14">2025-08-29</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Universal-adversarial-perturbations/">Universal adversarial perturbations</a></span></div><div class="content">英文题目：《Universal adversarial perturbations》 中文题目：《普遍对抗性扰动》 论文作者：Seyed-Mohsen Moosavi-Dezfooli,Alhussein Fawzi,Omar Fawzi &amp; Pascal Frossard 发布于：CV 发布时间：2017 Mar 9 级别：CCF-A 论文链接：  摘要 给出了一个最先进的深度神经网络分类器，我们证明了存在一个通用的(与图像无关的)非常小的扰动向量，它会导致自然图像以很高的概率被错误分类。我们提出了一个系统的算法来计算普遍的扰动，并表明最新的深度神经网络非常容易受到这种扰动的影响，尽管人眼是准不可感知的。我们进一步经验性地分析了这些普遍的扰动，并特别表明，它们在神经网络中具有很好的泛化能力。普遍扰动的惊人存在揭示了分类器高维决策边界之间的重要几何相关性。它进一步概述了输入空间中存在的单一方向的潜在安全漏洞，攻击者可能会利用这些方向来破坏大多数自然图像上的分类器。 本文聚焦的问题 近年来，图像分类器对结构化和非结构化扰动的鲁棒性受到广泛关注。尽管深度神经网络在视觉分类基准...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/08/29/%E6%98%93%E5%AD%90%E6%96%87/2025-08-30/One%20Pixel%20Attack%20for%20Fooling%20Deep%20Neural%20Networks/" title="One Pixel Attack for Fooling Deep Neural Networks">One Pixel Attack for Fooling Deep Neural Networks</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-08-29T15:04:14.000Z" title="发表于 2025-08-29 23:04:14">2025-08-29</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Attack/">Attack</a></span></div><div class="content">英文题目：《One Pixel Attack for Fooling Deep Neural Networks》 中文题目：《一种愚弄深度神经网络的像素攻击方法》 论文作者：Jiawei Su,Danilo Vasconcellos Vargas &amp; Kouichi Sakurai 发布于：LG 发布时间：2019 Oct 17 级别：CCF-A 论文链接：  摘要 最近的研究表明，通过向输入向量添加相对较小的扰动，可以很容易地改变深度神经网络(DNN)的输出。在本文中，我们分析了一个极其有限的场景下的攻击，其中只有一个像素可以被修改。为此，我们提出了一种新的基于差分进化的单像素对抗性扰动生成方法。由于DE的固有特性，它需要较少的敌意信息(黑盒攻击)，并且可以欺骗更多类型的网络。结果表明，在Kaggle CIFAR-10测试数据集和ImageNet(ILSVRC 2012)测试数据集中，67.97%的自然图像和16.04%的ImageNet(ILSVRC 2012)测试图像可以通过仅修改一个像素来扰动至少一个目标类，平均置信度分别为74.03%和22.91%。我们还在原始...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/08/29/%E5%8F%A4%E4%B9%89%E7%BF%94/2025-08-30/Jailbreaking%20Black%20Box%20Large%20Language%20Models%20in%20Twenty%20Queries/" title="Jailbreaking Black Box Large Language Models in Twenty Queries">Jailbreaking Black Box Large Language Models in Twenty Queries</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-08-29T11:58:14.000Z" title="发表于 2025-08-29 19:58:14">2025-08-29</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%B6%8A%E7%8B%B1%E6%94%BB%E5%87%BB/">越狱攻击</a></span></div><div class="content">英文题目：《Jailbreaking Black Box Large Language Models in Twenty Queries》 中文题目：《在 20 次查询内对黑盒大语言模型实施越狱攻击》 论文作者：  Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J.Pappas, Eric Wong 发布于： Computing Research Repository 发布时间：2023-10-12 级别：无 论文链接：https://cz5waila03cyo0tux1owpyofgoryroob.aminer.cn/27/D3/F0/27D3F04A17CE6E1DB47D32AE395B4A26.pdf 论文代码：  摘要 越来越多的人关注确保大型语言模型（LLM）与人类价值观保持一致。然而，这类模型的对齐容易受到对抗性越狱的影响，这会诱导LLM忽略其安全护栏。因此，识别这些漏洞对于理解内在的弱点并预防未来的滥用是至关重要的。为此，我们提出了Prompt Automatic Iter...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/08/29/%E6%98%93%E5%AD%90%E6%96%87/2025-08-30/DELVING%20INTO%20TRANSFERABLE%20ADVERSARIAL%20EXAMPLES%20AND%20BLACK%20-%20BOX%20ATTACKS/" title="DELVING INTO TRANSFERABLE ADVERSARIAL EXAMPLES AND BLACK - BOX ATTACKS">DELVING INTO TRANSFERABLE ADVERSARIAL EXAMPLES AND BLACK - BOX ATTACKS</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-08-29T02:58:14.000Z" title="发表于 2025-08-29 10:58:14">2025-08-29</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/BLACK-BOX-ATTACKS/">BLACK BOX ATTACKS</a></span></div><div class="content">英文题目：《DELVING INTO TRANSFERABLE ADVERSARIAL EXAMPLES AND BLACK - BOX ATTACKS》 中文题目：《深入研究可转移的对抗性例子和黑盒攻击》 论文作者：Yanpei Liu,Xinyun Chen,Chang Liu &amp; Dawn Song 发布于：ICLR 发布时间：2017 Feb 7 级别：CCF-A 论文链接：  摘要 深度神经网络的一个有趣的性质是存在对抗性的例子，这些例子可以在不同的体系结构之间转移。这些可转移的对抗性例子可能会严重阻碍基于神经网络的深度应用。以往的工作大多是使用小尺度数据集来研究可转移性。在这项工作中，我们首次对大规模模型和大规模数据集上的可转移性进行了广泛的研究，也首次研究了带有目标标签的目标对抗性实例的可转移性。我们研究了非目标对抗性实例和目标对抗性实例，并表明虽然可转移的非目标对抗性实例很容易找到，但使用现有方法生成的目标对抗性实例几乎不会与其目标标签一起转移。因此，我们提出了新的基于集成的方法来生成可转移的对抗性实例。使用这种方法，我们观察到很大比例的目标对抗性例子能够...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/08/29/%E5%8F%A4%E4%B9%89%E7%BF%94/2025-08-30/Voice%20Jailbreak%20Attacks%20Against%20GPT-4o/" title="Voice Jailbreak Attacks Against GPT-4o">Voice Jailbreak Attacks Against GPT-4o</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-08-29T01:58:14.000Z" title="发表于 2025-08-29 09:58:14">2025-08-29</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%B6%8A%E7%8B%B1%E6%94%BB%E5%87%BB/">越狱攻击</a></span></div><div class="content">英文题目：《Voice Jailbreak Attacks Against GPT-4o》 中文题目：《针对GPT-4o的语音越狱攻击》 论文作者：  Xinyue Shen, Yixin Wu, Michael Backes, Yang Zhang 发布于：Computing Research Repository 发布时间：2024-05-29 级别：无 论文链接： https://arxiv.org/abs/2405.19103 论文代码：  摘要 最近，人工智能助手的概念已从科幻小说走进现实应用。GPT-4o作为最新的跨音频、视觉和文本的多模态大语言模型（MLLM），通过实现更自然的人机交互，进一步模糊了虚构与现实之间的界限。然而，GPT-4o语音模式的出现也可能带来新的攻击面。在本文中，我们首次对针对GPT-4o语音模式的越狱攻击进行了系统性评估。我们发现，当将违禁问题和文本越狱提示直接转换为语音模式时，GPT-4o对其表现出良好的抗性。这种抗性主要源于GPT-4o的内部防护机制以及将文本越狱提示适配到语音模式的难度。受GPT-4o类似人类行为的启发，我们提出了Voic...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/08/27/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-08-30/WordGame%20Efficient%20&amp;%20Effective%20LLM%20Jailbreak%20via%20Simultaneous%20Obfuscation%20in%20Query%20and%20Response/" title="WordGame: Efficient &amp; Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response">WordGame: Efficient &amp; Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-08-27T08:58:14.000Z" title="发表于 2025-08-27 16:58:14">2025-08-27</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%B6%8A%E7%8B%B1%E6%94%BB%E5%87%BB/">越狱攻击</a></span></div><div class="content">英文题目：《WordGame: Efficient &amp; Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response》 中文题目：《WordGame：基于查询与响应混淆的大语言模型高效越狱攻击方法》 论文作者：Tianrong Zhang, Bochuan Cao, Yuanpu Cao, Lu Lin, Prasenjit Mitra, Jinghui Chen 发布于： arxiv 发布时间：2024-05-22 级别：无 论文链接：https://doi.org/10.48550/arXiv.2405.14023 论文代码：无  摘要 近期，诸如 ChatGPT 等大型语言模型（LLM）取得的重大突破以前所未有的速度革新了生产流程。与此同时，人们也越来越担忧 LLM 容易遭受破解攻击，从而生成有害或不安全的内容。尽管已经在 LLM 中实施了安全对齐措施来减轻现有的破解尝试，并使其变得越来越复杂，但这些措施仍远非完美。在本文中，我们分析了当前安全对齐的常见模式，并表明可以通过在查询...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/08/26/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-08-30/Enhancing%20Jailbreak%20Attacks%20on%20LLMs%20via%20Persona%20Prompts/" title="Enhancing Jailbreak Attacks on LLMs via Persona Prompts">Enhancing Jailbreak Attacks on LLMs via Persona Prompts</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-08-26T07:58:14.000Z" title="发表于 2025-08-26 15:58:14">2025-08-26</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%B6%8A%E7%8B%B1%E6%94%BB%E5%87%BB/">越狱攻击</a></span></div><div class="content">英文题目：《Enhancing Jailbreak Attacks on LLMs via Persona Prompts》 中文题目：《通过角色提示增强大型语言模型（LLMs）的越狱攻击》 论文作者：Zheng Zhang, Peilin Zhao, Deheng Ye, Hao Wang 发布于： arxiv 发布时间：2024-07-28 级别：无 论文链接： https://doi.org/10.48550/arXiv.2507.22171 论文代码：https://github.com/CjangCjengh/Generic_Persona  摘要 越狱攻击旨在通过诱导大型语言模型（LLMs）生成有害内容来利用其漏洞，进而揭示模型的安全缺陷。理解并应对此类攻击对于推动 LLM 安全领域发展至关重要。以往的越狱方法主要聚焦于对有害意图的直接操纵，却较少关注角色提示（persona prompts）的影响。本研究系统探究了角色提示在突破 LLM 防御机制中的有效性，提出一种基于遗传算法的方法，可自动生成角色提示以绕过 LLM 的安全机制。实验结果表明：（1）经进化生成的角色...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/08/25/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-08-30/PRISM%20Programmatic%20Reasoning%20with%20Image%20Sequence%20Manipulation%20for%20LVLM%20Jailbreaking/" title="PRISM: Programmatic Reasoning with Image Sequence Manipulation for LVLM Jailbreaking">PRISM: Programmatic Reasoning with Image Sequence Manipulation for LVLM Jailbreaking</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-08-25T02:58:14.000Z" title="发表于 2025-08-25 10:58:14">2025-08-25</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/%E8%B6%8A%E7%8B%B1%E6%94%BB%E5%87%BB/">越狱攻击</a></span></div><div class="content">英文题目：《PRISM: Programmatic Reasoning with Image Sequence Manipulation for LVLM Jailbreaking》 中文题目：《PRISM：面向大型视觉语言模型（LVLM）越狱的、基于图像序列操纵的程序化推理》 论文作者：Quanchen Zou, Zonghao Ying, Moyang Chen, Wenzhuo Xu, Yisong Xiao, Yakai Li, Deyue Zhang, Dongdong Yang, Zhao Liu, Xiangzheng Zhang 发布于： arxiv 发布时间：2025-07-29 级别：无 论文链接： https://doi.org/10.48550/arXiv.2507.21540 论文代码：无  摘要 大型视觉语言模型（LVLMs）的复杂程度不断提升，与此同时，旨在防止生成有害内容的安全对齐机制也在逐步发展。然而，这些防御机制在复杂的对抗性攻击面前仍显脆弱。现有越狱方法通常依赖直接且语义明确的提示词，却忽视了大型视觉语言模型（LVLMs）在多步推理过程中整合...</div></div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/3/#content-inner"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/#content-inner">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/#content-inner">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/#content-inner">8</a><a class="extend next" rel="next" href="/page/5/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">LLM Security Group</div><div class="author-info-description">分享知识，认识世界</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">77</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">81</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">28</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/26/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-10-27/Multi-Turn%20Jailbreaking%20Large%20Language%20Models%20via%20Attention%20Shifting/" title="Multi-Turn Jailbreaking Large Language Models via Attention Shifting">Multi-Turn Jailbreaking Large Language Models via Attention Shifting</a><time datetime="2025-10-26T04:58:14.000Z" title="发表于 2025-10-26 12:58:14">2025-10-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/26/%E5%8F%A4%E4%B9%89%E7%BF%94/2025-10-27/Images%20are%20Achilles%E2%80%99%20Heel%20of%20Alignment%20Exploiting%20Visual%20Vulnerabilities%20for%20Jailbreaking%20Multimodal%20Large%20Language%20Models/" title="Images are Achilles’ Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models">Images are Achilles’ Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models</a><time datetime="2025-10-25T16:00:00.000Z" title="发表于 2025-10-26 00:00:00">2025-10-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/25/%E5%8F%A4%E4%B9%89%E7%BF%94/2025-10-27/Distraction%20is%20All%20You%20Need%20for%20Multimodal%20Large%20Language%20Model%20Jailbreaking/" title="Distraction is All You Need for Multimodal Large Language Model Jailbreaking">Distraction is All You Need for Multimodal Large Language Model Jailbreaking</a><time datetime="2025-10-24T16:00:00.000Z" title="发表于 2025-10-25 00:00:00">2025-10-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/21/%E4%BC%8D%E4%BF%8A/2025-10-24/SIDA%20Social%20Media%20Image%20Deepfake%20Detection,%20Localization%20and%20Explanation/" title="SIDA: Social Media Image Deepfake Detection, Localization and Explanation with Large Multimodal Model">SIDA: Social Media Image Deepfake Detection, Localization and Explanation with Large Multimodal Model</a><time datetime="2025-10-21T11:53:16.000Z" title="发表于 2025-10-21 19:53:16">2025-10-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/21/%E4%BC%8D%E4%BF%8A/2025-10-24/Generalized%20Diffusion%20Detector%20Mining%20Robust%20Features%20from%20Diffusion%20Models%20%20for%20Domain-Generalized%20Detection/" title="Generalized Diffusion Detector Mining Robust Features from Diffusion Models  for Domain-Generalized Detection">Generalized Diffusion Detector Mining Robust Features from Diffusion Models  for Domain-Generalized Detection</a><time datetime="2025-10-21T11:53:16.000Z" title="发表于 2025-10-21 19:53:16">2025-10-21</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            <a class="card-more-btn" href="/categories/" title="查看更多">
      <i class="fas fa-angle-right"></i></a>
          </div>
          <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/ADVERSARIAL-DEFENSE/"><span class="card-category-list-name">ADVERSARIAL DEFENSE</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/AI%E7%B3%BB%E7%BB%9F%E4%BC%98%E5%8C%96/"><span class="card-category-list-name">AI系统优化</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Adversarial/"><span class="card-category-list-name">Adversarial</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Adversarial-Text-Generation/"><span class="card-category-list-name">Adversarial Text Generation</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Adversarial-attack/"><span class="card-category-list-name">Adversarial attack</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Attack/"><span class="card-category-list-name">Attack</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/BLACK-BOX-ATTACKS/"><span class="card-category-list-name">BLACK BOX ATTACKS</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/High-Confidence-Predictions-for-Unrecognizable-Images/"><span class="card-category-list-name">High Confidence Predictions for Unrecognizable Images</span><span class="card-category-list-count">1</span></a></li>
          </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/%E5%8F%8C%E8%B6%85%E5%9B%BE%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 1.1em; color: #999">双超图卷积网络</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 1.5em; color: #99a9bf">强化学习</a> <a href="/tags/%E7%BC%96%E7%A0%81%E5%99%A8%E8%A7%A3%E7%A0%81%E5%99%A8/" style="font-size: 1.1em; color: #999">编码器解码器</a> <a href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%BC%98%E5%8C%96/" style="font-size: 1.1em; color: #999">损失函数优化</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E4%BC%AA%E9%80%A0%E6%A3%80%E6%B5%8B/" style="font-size: 1.1em; color: #999">人脸伪造检测</a> <a href="/tags/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93/" style="font-size: 1.1em; color: #999">多智能体</a> <a href="/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%A3/" style="font-size: 1.1em; color: #999">注意力分散</a> <a href="/tags/%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83/" style="font-size: 1.1em; color: #999">监督微调</a> <a href="/tags/%E5%A4%A7%E5%9E%8B%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B/" style="font-size: 1.37em; color: #99a4b2">大型多模态模型</a> <a href="/tags/%E5%B1%82%E6%AC%A1%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88/" style="font-size: 1.1em; color: #999">层次特征融合</a> <a href="/tags/%E5%9F%BA%E6%9C%AC%E8%BF%AD%E4%BB%A3%E6%B3%95/" style="font-size: 1.1em; color: #999">基本迭代法</a> <a href="/tags/A3C%E7%AE%97%E6%B3%95/" style="font-size: 1.23em; color: #999ea6">A3C算法</a> <a href="/tags/%E7%89%B9%E5%BE%81%E5%A2%9E%E5%BC%BA/" style="font-size: 1.1em; color: #999">特征增强</a> <a href="/tags/GCG%E4%BC%98%E5%8C%96/" style="font-size: 1.1em; color: #999">GCG优化</a> <a href="/tags/RapidFuzz/" style="font-size: 1.1em; color: #999">RapidFuzz</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87/" style="font-size: 1.1em; color: #999">梯度上升</a> <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8/" style="font-size: 1.37em; color: #99a4b2">大模型安全</a> <a href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/" style="font-size: 1.1em; color: #999">遗传算法</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E4%BC%AA%E9%80%A0%E5%AE%9A%E4%BD%8D/" style="font-size: 1.23em; color: #999ea6">图像伪造定位</a> <a href="/tags/adversarial-example/" style="font-size: 1.1em; color: #999">adversarial example</a> <a href="/tags/PSA/" style="font-size: 1.1em; color: #999">PSA</a> <a href="/tags/%E5%AE%89%E5%85%A8%E5%AF%B9%E9%BD%90/" style="font-size: 1.23em; color: #999ea6">安全对齐</a> <a href="/tags/%E5%90%8E%E9%97%A8%E6%94%BB%E5%87%BB/" style="font-size: 1.37em; color: #99a4b2">后门攻击</a> <a href="/tags/Adversarial-attack-ransfer-based-attack/" style="font-size: 1.1em; color: #999">Adversarial attack,ransfer-based attack</a> <a href="/tags/%E8%81%9A%E7%B1%BB/" style="font-size: 1.1em; color: #999">聚类</a> <a href="/tags/Search-R1/" style="font-size: 1.1em; color: #999">Search-R1</a> <a href="/tags/%E5%BE%AE%E8%B0%83/" style="font-size: 1.23em; color: #999ea6">微调</a> <a href="/tags/State-Space-Models/" style="font-size: 1.1em; color: #999">State Space Models</a> <a href="/tags/%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%9B%BE%E5%83%8F%E4%BC%AA%E9%80%A0%E6%A3%80%E6%B5%8B/" style="font-size: 1.1em; color: #999">弱监督图像伪造检测</a> <a href="/tags/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E4%BC%AA%E9%80%A0%E6%A3%80%E6%B5%8B/" style="font-size: 1.23em; color: #999ea6">可解释性伪造检测</a> <a href="/tags/%E5%99%AA%E5%A3%B0%E5%BC%95%E5%AF%BC%E7%BD%91%E7%BB%9C/" style="font-size: 1.1em; color: #999">噪声引导网络</a> <a href="/tags/%E9%9F%B3%E9%A2%91%E8%B6%8A%E7%8B%B1%E6%94%BB%E5%87%BB/" style="font-size: 1.1em; color: #999">音频越狱攻击</a> <a href="/tags/%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95/" style="font-size: 1.1em; color: #999">进化算法</a> <a href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E5%AF%B9%E9%BD%90/" style="font-size: 1.1em; color: #999">大模型安全对齐</a> <a href="/tags/%E5%99%AA%E5%A3%B0%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/" style="font-size: 1.1em; color: #999">噪声表示学习</a> <a href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" style="font-size: 1.23em; color: #999ea6">多模态大型语言模型</a> <a href="/tags/%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88/" style="font-size: 1.1em; color: #999">特征融合</a> <a href="/tags/%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%8D%8F%E4%BD%9C/" style="font-size: 1.37em; color: #99a4b2">多智能体协作</a> <a href="/tags/PRISM/" style="font-size: 1.1em; color: #999">PRISM</a> <a href="/tags/JailFuzzer/" style="font-size: 1.1em; color: #999">JailFuzzer</a></div></div><div class="card-widget card-archives">
    <div class="item-headline">
      <i class="fas fa-archive"></i>
      <span>归档</span>
      
    </div>
  
    <ul class="card-archive-list">
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/10/">
            <span class="card-archive-list-date">
              十月 2025
            </span>
            <span class="card-archive-list-count">17</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/09/">
            <span class="card-archive-list-date">
              九月 2025
            </span>
            <span class="card-archive-list-count">14</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/08/">
            <span class="card-archive-list-date">
              八月 2025
            </span>
            <span class="card-archive-list-count">45</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2024/12/">
            <span class="card-archive-list-date">
              十二月 2024
            </span>
            <span class="card-archive-list-count">1</span>
          </a>
        </li>
      
    </ul>
  </div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站信息</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">77</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总浏览量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-10-27T02:27:54.130Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By LLM Security Group</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.3</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script src="https://cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/DynamicRibbon.min.js"></script><script src="https://cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/star.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="输入关键词…" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>