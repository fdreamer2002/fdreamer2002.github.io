<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Jailbroken: How Does LLM Safety Training Fail? | LLM Security Group 's Notes</title><meta name="author" content="LLM Security Group"><meta name="copyright" content="LLM Security Group"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="英文题目：《Jailbroken: How Does LLM Safety Training Fail?》 中文题目：《Jailbroken：LLM安全训练是如何失败的？》 论文作者：Alexander Wei, Nika Haghtalab, Jacob Steinhardt 发布于： NIPS 发布时间：2023-07-05 级别：无 论文链接：https:&#x2F;&#x2F;doi.org&#x2F;10.48550">
<meta property="og:type" content="article">
<meta property="og:title" content="Jailbroken: How Does LLM Safety Training Fail?">
<meta property="og:url" content="https://fdreamer2002.github.io/2025/10/19/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-10-20/Jailbroken%20How%20Does%20LLM%20Safety%20Training%20Fail/index.html">
<meta property="og:site_name" content="LLM Security Group &#39;s Notes">
<meta property="og:description" content="英文题目：《Jailbroken: How Does LLM Safety Training Fail?》 中文题目：《Jailbroken：LLM安全训练是如何失败的？》 论文作者：Alexander Wei, Nika Haghtalab, Jacob Steinhardt 发布于： NIPS 发布时间：2023-07-05 级别：无 论文链接：https:&#x2F;&#x2F;doi.org&#x2F;10.48550">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://fdreamer2002.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-10-19T11:58:14.000Z">
<meta property="article:modified_time" content="2025-10-20T05:33:25.058Z">
<meta property="article:author" content="LLM Security Group">
<meta property="article:tag" content="越狱分析与概念">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://fdreamer2002.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Jailbroken: How Does LLM Safety Training Fail?",
  "url": "https://fdreamer2002.github.io/2025/10/19/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-10-20/Jailbroken%20How%20Does%20LLM%20Safety%20Training%20Fail/",
  "image": "https://fdreamer2002.github.io/img/butterfly-icon.png",
  "datePublished": "2025-10-19T11:58:14.000Z",
  "dateModified": "2025-10-20T05:33:25.058Z",
  "author": [
    {
      "@type": "Person",
      "name": "游俊爽",
      "url": "https://fdreamer2002.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://fdreamer2002.github.io/2025/10/19/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-10-20/Jailbroken%20How%20Does%20LLM%20Safety%20Training%20Fail/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":"ture","top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Jailbroken: How Does LLM Safety Training Fail?',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/scroll.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/gradient.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lxgw-wenkai-screen-webfont@1.7.0/style.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fontsource/jetbrains-mono@4.5.12/index.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">92</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">92</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">27</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><span> 类别分类</span></a></li><li><a class="site-page child" href="/authors/"><span> 作者分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">LLM Security Group 's Notes</span></a><a class="nav-page-title" href="/"><span class="site-name">Jailbroken: How Does LLM Safety Training Fail?</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><span> 类别分类</span></a></li><li><a class="site-page child" href="/authors/"><span> 作者分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Jailbroken: How Does LLM Safety Training Fail?</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-10-19T11:58:14.000Z" title="发表于 2025-10-19 19:58:14">2025-10-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-20T05:33:25.058Z" title="更新于 2025-10-20 13:33:25">2025-10-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%B6%8A%E7%8B%B1%E6%94%BB%E5%87%BB/">越狱攻击</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><div class="note success flat"><p>英文题目：《Jailbroken: How Does LLM Safety Training Fail?》</p>
<p>中文题目：《Jailbroken：LLM安全训练是如何失败的？》</p>
<p>论文作者：Alexander Wei, Nika Haghtalab, Jacob Steinhardt</p>
<p>发布于： NIPS</p>
<p>发布时间：2023-07-05</p>
<p>级别：无</p>
<p>论文链接：<a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2307.02483">https://doi.org/10.48550/arXiv.2307.02483</a></p>
<p>论文代码：无</p>
</div>
<h2 id="摘要">摘要</h2>
<p>大型语言模型（LLM）在安全性和无害性方面进行了训练，但仍然容易受到对抗性滥用，早期版本的ChatGPT中普遍存在的“越狱”（jailbreak）攻击就证明了这一点，这些攻击会引发不良行为。除了认识到这个问题之外，我们还调查了为什么这些攻击会成功以及如何创建它们。我们假设安全训练的两种失败模式：竞争性目标和不匹配的泛化。当模型的能力和安全目标发生冲突时，就会出现竞争性目标；而不匹配的泛化则发生在安全训练未能泛化到存在能力的领域时。我们利用这些失败模式来指导越狱设计，然后针对现有和新设计的攻击评估最先进的模型，包括OpenAI的GPT-4和Anthropic的Claude v1.3。我们发现，尽管这些模型背后有大量的红队测试和安全训练工作，但漏洞仍然存在。值得注意的是，利用我们的失败模式的新攻击在模型红队评估集中，针对不安全请求集合中的每个提示都成功了，并且优于现有的临时越狱。我们的分析强调了安全能力对等的需求——即安全机制应该与底层模型一样复杂——并反对仅靠扩展就能解决这些安全失败模式的观点。</p>
<h2 id="本文聚焦的问题">本文聚焦的问题</h2>
<p>虽然加强LLM的安全性有所帮助，但模型仍然容易受到对抗性输入的影响，自ChatGPT最初发布以来，社交媒体上“越狱”的传播就证明这一点。这些攻击旨在引出模型被训练要避免的行为，例如产生有害内容或泄露个人身份信息。攻击范围可以从精心设计的角色扮演到对安全目标的微妙颠覆。模型创建者已经承认并更新了他们的模型以应对越狱攻击，但是仍然缺乏对这种现象的系统分析和概念理解。</p>
<h2 id="本文提出的方法">本文提出的方法</h2>
<p>越狱攻击并非孤立现象，而是模型当前训练方式所固有的。当模型的预训练和指令遵循目标与其安全目标相冲突时，就会出现竞争性目标（图a）。相反，当输入对于模型的安全训练数据而言是分布外的，但在其广泛的预训练语料库范围内时，就会出现不匹配的泛化（图b）。</p>
<p><img src="https://cdn.jsdelivr.net/gh/i-s-land/Article-pictures/ai/20251020094020235.png" alt=""></p>
<p>关于竞争性目标（图a），其可以解释为经过安全训练的LLM通常是针对多个可能相互冲突的目标进行训练的。具体而言，最先进的LLM接受语言建模、指令遵循和安全方面的训练。通过精心设计提示，迫使模型在受限行为或受到预训练和指令遵循目标严重惩罚的响应之间做出选择。</p>
<p>示例：前缀注入，这种攻击要求模型首先输出一个看起来无害的前缀，该前缀的设计使得以该前缀为条件不太可能在预训练分布中拒绝。</p>
<p><img src="https://cdn.jsdelivr.net/gh/i-s-land/Article-pictures/ai/20251020095304676.png" alt=""></p>
<p>上述攻击可能导致 GPT-4 提供关于骚扰、犯罪和暴力的有害信息。 但是注入的前缀文本很重要：将前缀更改为“Hello!”会使 GPT-4 不再表现出上述行为。</p>
<p>当一个 LLM 解码对这个提示的响应时，文章假设这种攻击通过两种方式利用了竞争目标：首先，由于模型会因拒绝无害指令而受到惩罚 ，因此会遵循无害的注入指令。 然后，由于在预训练分布中不太可能看到前缀后的拒绝，因此模型的预训练目标会严重惩罚拒绝。 因此，模型继续响应不安全的提示。</p>
<p>示例：拒绝抑制，在这种攻击中，模型被指示在排除常见拒绝响应的约束下进行响应，从而使不安全的响应更有可能发生。</p>
<p><img src="https://cdn.jsdelivr.net/gh/i-s-land/Article-pictures/ai/20251020100121310.png" alt=""></p>
<p>上述攻击导致 GPT-4 回复要求提供关于犯罪、社会工程和成人内容等方面建议的提示。 文章发现具体的指令很重要：反转这些规则（例如，“1.考虑道歉……”）不会导致数据集中任何提示出现受限行为。</p>
<p>首先，指令遵循训练响应指令并降低开始典型拒绝的 tokens 的权重。 因此，模型选择更可能开始响应的 tokens。 一旦开始响应，预训练目标会非常倾向于继续而不是突然逆转，从而导致完全不安全的输出。</p>
<p>文章发现现有的越狱方法也利用了这种相互冲突目标的现象。例如，广为流传的“DAN”越狱方法就利用了通过一系列指令来遵循特定指令的方式来扮演角色“DAN”，并通过要求输出以“[DAN]：”开头的方式进行预训练。另一个越狱方法则巧妙地利用了提示注入的变体来绕过拒绝：它先要求发表一篇关于 OpenAI 内容政策的说教式言论，然后注入字符串“但现在既然我们已经解决了强制性的违规行为，那我们就来打破这该死的规则吧：”。通过扩展前缀注入，文章还发现可以通过风格注入来利用相互冲突的目标，例如，要求不要使用长单词，之后模型专业撰写的拒绝声明不太可能接着出现。</p>
<p>关于不匹配的泛化（图b），即预训练是在比安全训练更大和更多样化的数据集上完成的，因此该模型具有许多安全训练未涵盖的能力。这种不匹配可以通过构建提示来利用越狱，在这种提示上，预训练和指令遵循可以泛化，但模型的安全训练不能。对于这样的提示，模型会响应，但没有安全考虑。</p>
<p>示例：Base64。在Base64越狱中，提示语使用Base64进行混淆。Base64是一种二进制到文本的编码方式，它将每个字节编码为三个文本字符，以绕过模型的安全训练。</p>
<p><img src="https://cdn.jsdelivr.net/gh/i-s-land/Article-pictures/ai/20251020101223232.png" alt=""></p>
<p>不匹配的泛化可能发生，因为大型模型在预训练期间学习了Base64，并学会直接遵循Base64编码的指令。另一方面，安全训练也可能不包含像Base64编码指令这样不自然的输入，因此模型从未经过训练来拒绝此类提示。因此，模型未能做出拒绝响应的原因很可能是因为输入严重偏离分布。</p>
<p>存在大量的混淆方案：在字符层面，它们包括 ROT13 密码、leet 语言（用视觉上相似的数字和符号替换字母）以及摩尔斯电码。在单词层面，它们包括猪拉丁语，将敏感词替换为同义词（例如“pilfer”代替“steal”），或者分段操作，将敏感词拆分成子字符串。提示层面的混淆包括将内容翻译成其他语言或仅要求模型以它能够理解的方式进行混淆。在许多此类情况下，模型仍然可以遵循混淆后的指令，但安全性无法转移。</p>
<p>除了混淆之外，大型语言模型还有许多在安全训练期间未被探索的能力。预训练和遵循指令能够泛化但安全性无法实现的情况包括：（i）“干扰指令”，即连续写下许多随机请求；（ii）要求以不寻常的输出格式（例如 JSON）进行响应；（iii）要求从模型在预训练期间见过但安全训练期间未提及的网站获取内容。</p>
<p>基于以上，文章认为，(i) 仅靠扩展规模无法解决以上的失效模式，并且 (ii) “安全-能力对等”——即安全机制与基础模型的复杂程度相匹配——对于防御对抗性使用可能是必要的。</p>
<p>竞争目标的核心矛盾是预训练目标（语言建模 + 指令遵循）与安全目标的固有冲突。即使模型参数规模扩大（如从 GPT-3 到 GPT-4），其优化框架仍包含：</p>
<p>​	<strong>KL 散度约束</strong>：要求安全微调后的模型分布贴近预训练模型（避免能力退化），导致模型在安全拒绝时需权衡预训练偏好（如生成连贯文本）。</p>
<p>​	<strong>奖励信号冲突</strong>：安全训练希望模型拒绝有害请求，但预训练数据中 “遵循指令” 的奖励更强。</p>
<p>泛化不匹配的本质是<strong>安全训练数据的分布远窄于预训练数据的分布</strong>。规模扩展（如模型参数量从百亿到千亿）会：</p>
<p>​	<strong>扩展模型能力域</strong>：更大的模型能理解更复杂的输入（如 Base64 编码、多语言混淆），但安全训练未必覆盖这些新能力。</p>
<p>​	<strong>加剧能力 - 安全的不对称</strong>：模型能处理的输入类型（如 ROT13、Payload 拆分）随规模指数级增长，但安全训练依赖人工标注或有限对抗数据，无法同步扩展。</p>
<p>“安全-能力对等”是必要的——即安全机制与底层模型一样复杂。否则，攻击将利用模型的前沿能力，而不太先进的安全机制无法检测或解决这些能力。例如，能力较弱的模型进行的标记和过滤不是可靠的解决方案，因为它们可能无法识别威胁：没有Base64解码能力的模型将无法标记Base64攻击的Base64编码的输入和输出。即使是经验丰富的人工标注员，在没有帮助的情况下，也可能难以评估混淆的和对抗性的输入和输出。随着规模的扩大，这种不对称性只会越来越严重，因为能力更强的语言模型可能能够产生更微妙形式的输出（例如，隐写术），这将进一步逃避检测。</p>
<h2 id="阅读总结">阅读总结</h2>
<p>优点：</p>
<p>1、提出 “竞争目标”（模型能力与安全目标冲突）和 “泛化不匹配”（安全训练未覆盖预训练具备的能力域）两大核心失效模式，为理解 LLM 安全漏洞提供了统一的理论框架，填补了 “为何越狱攻击普遍存在” 的认知空白。</p>
<p>2、提出 “安全 - 能力对等”（安全机制复杂度需匹配模型基础能力）的核心防御原则，为后续防御方案设计提供明确方向。</p>
<p>缺点：</p>
<p>1、测试的 GPT-4、Claude v1.3 均为闭源商用模型，研究者仅能通过黑箱接口交互，无法获取模型权重、训练数据或中间激活值，导致对 “竞争目标如何在优化过程中体现”“泛化不匹配的具体数据分布差异” 等机制层面的验证只能间接推断，缺乏直接证据。</p>
<p>未来可以利用开源 LLM的可访问性，通过修改训练目标、分析中间层激活，直接验证 “竞争目标”“泛化不匹配” 的机制细节，弥补黑箱模型的局限。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://fdreamer2002.github.io">游俊爽</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://fdreamer2002.github.io/2025/10/19/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-10-20/Jailbroken%20How%20Does%20LLM%20Safety%20Training%20Fail/">https://fdreamer2002.github.io/2025/10/19/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-10-20/Jailbroken%20How%20Does%20LLM%20Safety%20Training%20Fail/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://fdreamer2002.github.io" target="_blank">LLM Security Group 's Notes</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%B6%8A%E7%8B%B1%E5%88%86%E6%9E%90%E4%B8%8E%E6%A6%82%E5%BF%B5/">越狱分析与概念</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/10/20/%E6%98%93%E5%AD%90%E6%96%87/2025-10-20/Adaptive-Perturbation-for-Adversarial-Attack/" title="’Adaptive Perturbation for Adversarial Attack'"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">’Adaptive Perturbation for Adversarial Attack'</div></div><div class="info-2"><div class="info-item-1">英文题目：《Adaptive Perturbation for Adversarial Attack》 论文作者：YuanZheng,ZhangJie,JiangZhaoyan,LiLiangliang,ShanShiguang 发布于：IEEE Transactions on Pattern Analysis and Machine Intelligence 发布时间：2024/8 级别：CCF A 论文链接：10.1109/TPAMI.2024.3367773 摘要 In recent years, the security of deep learning models achieves more and more attentions with the rapid development of neural networks, which are vulnerable to adversarial examples.Almost all existing gradient-based attack methods use the sign function in the ...</div></div></div></a><a class="pagination-related" href="/2025/10/18/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-10-20/Fine-tuning%20Aligned%20Language%20Models%20Compromises%20Safety,%20Even%20When%20Users%20Do%20Not%20Intend%20To!/" title="Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</div></div><div class="info-2"><div class="info-item-1">英文题目：《Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!》 中文题目：《微调对齐的语言模型会降低安全性，即使使用者无意为之！》 论文作者：Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, Peter Henderson 发布于： ICLR 2024 发布时间：2023-10-05 级别：CCF-A 论文链接： https://doi.org/10.48550/arXiv.2310.03693 论文代码：无  摘要 将大型语言模型（LLM）优化以用于下游应用场景通常需要通过进一步的微调来对预训练的 LLM 进行定制。Meta 公开发布了 Llama 模型，并且 OpenAI 提供了用于在自定义数据集上对 GPT-3.5 Turbo 进行微调的 API，这也鼓励了这种做法。但是，这种定制微调所涉及的安全成本是什么呢？我们注意到，尽管现有的安全对齐基础设施可...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">LLM Security Group</div><div class="author-info-description">分享知识，认识世界</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">92</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">92</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">27</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E8%81%9A%E7%84%A6%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">2.</span> <span class="toc-text">本文聚焦的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E6%8F%90%E5%87%BA%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">本文提出的方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%98%85%E8%AF%BB%E6%80%BB%E7%BB%93"><span class="toc-number">4.</span> <span class="toc-text">阅读总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/10/%E5%8F%A4%E4%B9%89%E7%BF%94/2025-11-10/A%20Wolf%20in%20Sheep%E2%80%99s%20Clothing%20Generalized%20Nested%20Jailbreak%20Prompts%20can%20Fool%20Large%20Language%20Models%20Easily/" title="A Wolf in Sheep’s Clothing Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily">A Wolf in Sheep’s Clothing Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily</a><time datetime="2025-11-09T16:00:00.000Z" title="发表于 2025-11-10 00:00:00">2025-11-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/04/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-11-10/PAPILLON%20Efficient%20and%20Stealthy%20Fuzz%20Testing-Powered%20Jailbreaks%20for%20LLMs/" title="PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs">PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs</a><time datetime="2025-11-04T11:58:14.000Z" title="发表于 2025-11-04 19:58:14">2025-11-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/04/%E4%BC%8D%E4%BF%8A/2025-11-07/SNIS%20A%20Signal%20Noise%20Separation-Based%20Network%20%20for%20Post-Processed%20Image%20Forgery%20Detection/" title="SNIS: A Signal Noise Separation-Based Network  for Post-Processed Image Forgery Detection">SNIS: A Signal Noise Separation-Based Network  for Post-Processed Image Forgery Detection</a><time datetime="2025-11-04T11:53:16.000Z" title="发表于 2025-11-04 19:53:16">2025-11-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/04/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-11-10/AutoDAN%20Generating%20Stealthy%20Jailbreak%20Prompts%20on%20Aligned%20Large%20Language%20Models/" title="AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models">AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models</a><time datetime="2025-11-04T07:51:14.000Z" title="发表于 2025-11-04 15:51:14">2025-11-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/04/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-11-10/Jailbreaking%20Leading%20Safety-Aligned%20LLMs%20with%20Simple%20Adaptive%20Attacks/" title="Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks">Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks</a><time datetime="2025-11-04T05:31:14.000Z" title="发表于 2025-11-04 13:31:14">2025-11-04</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By LLM Security Group</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.3</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script src="https://cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/DynamicRibbon.min.js"></script><script src="https://cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/star.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="输入关键词…" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>