<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Enhancing Jailbreak Attacks on LLMs via Persona Prompts | LLM Security Group 's Notes</title><meta name="author" content="LLM Security Group"><meta name="copyright" content="LLM Security Group"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="英文题目：《Enhancing Jailbreak Attacks on LLMs via Persona Prompts》 中文题目：《通过角色提示增强大型语言模型（LLMs）的越狱攻击》 论文作者：Zheng Zhang, Peilin Zhao, Deheng Ye, Hao Wang 发布于： arxiv 发布时间：2024-07-28 级别：无 论文链接： https:&#x2F;&#x2F;doi.org">
<meta property="og:type" content="article">
<meta property="og:title" content="Enhancing Jailbreak Attacks on LLMs via Persona Prompts">
<meta property="og:url" content="https://fdreamer2002.github.io/2025/08/26/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-08-30/Enhancing%20Jailbreak%20Attacks%20on%20LLMs%20via%20Persona%20Prompts/index.html">
<meta property="og:site_name" content="LLM Security Group &#39;s Notes">
<meta property="og:description" content="英文题目：《Enhancing Jailbreak Attacks on LLMs via Persona Prompts》 中文题目：《通过角色提示增强大型语言模型（LLMs）的越狱攻击》 论文作者：Zheng Zhang, Peilin Zhao, Deheng Ye, Hao Wang 发布于： arxiv 发布时间：2024-07-28 级别：无 论文链接： https:&#x2F;&#x2F;doi.org">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://fdreamer2002.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-08-26T07:58:14.000Z">
<meta property="article:modified_time" content="2025-09-01T05:31:23.947Z">
<meta property="article:author" content="LLM Security Group">
<meta property="article:tag" content="遗传算法">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://fdreamer2002.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Enhancing Jailbreak Attacks on LLMs via Persona Prompts",
  "url": "https://fdreamer2002.github.io/2025/08/26/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-08-30/Enhancing%20Jailbreak%20Attacks%20on%20LLMs%20via%20Persona%20Prompts/",
  "image": "https://fdreamer2002.github.io/img/butterfly-icon.png",
  "datePublished": "2025-08-26T07:58:14.000Z",
  "dateModified": "2025-09-01T05:31:23.947Z",
  "author": [
    {
      "@type": "Person",
      "name": "游俊爽",
      "url": "https://fdreamer2002.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://fdreamer2002.github.io/2025/08/26/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-08-30/Enhancing%20Jailbreak%20Attacks%20on%20LLMs%20via%20Persona%20Prompts/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":"ture","top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Enhancing Jailbreak Attacks on LLMs via Persona Prompts',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/scroll.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/gradient.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lxgw-wenkai-screen-webfont@1.7.0/style.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fontsource/jetbrains-mono@4.5.12/index.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">92</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">92</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">27</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><span> 类别分类</span></a></li><li><a class="site-page child" href="/authors/"><span> 作者分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">LLM Security Group 's Notes</span></a><a class="nav-page-title" href="/"><span class="site-name">Enhancing Jailbreak Attacks on LLMs via Persona Prompts</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><span> 类别分类</span></a></li><li><a class="site-page child" href="/authors/"><span> 作者分类</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Enhancing Jailbreak Attacks on LLMs via Persona Prompts</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-26T07:58:14.000Z" title="发表于 2025-08-26 15:58:14">2025-08-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-01T05:31:23.947Z" title="更新于 2025-09-01 13:31:23">2025-09-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%B6%8A%E7%8B%B1%E6%94%BB%E5%87%BB/">越狱攻击</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><div class="note success flat"><p>英文题目：《Enhancing Jailbreak Attacks on LLMs via Persona Prompts》</p>
<p>中文题目：《通过角色提示增强大型语言模型（LLMs）的越狱攻击》</p>
<p>论文作者：Zheng Zhang, Peilin Zhao, Deheng Ye, Hao Wang</p>
<p>发布于： arxiv</p>
<p>发布时间：2024-07-28</p>
<p>级别：无</p>
<p>论文链接： <a target="_blank" rel="noopener" href="https://doi.org/10.48550/arXiv.2507.22171">https://doi.org/10.48550/arXiv.2507.22171</a></p>
<p>论文代码：<a target="_blank" rel="noopener" href="https://github.com/CjangCjengh/Generic_Persona">https://github.com/CjangCjengh/Generic_Persona</a></p>
</div>
<h2 id="摘要">摘要</h2>
<p>越狱攻击旨在通过诱导大型语言模型（LLMs）生成有害内容来利用其漏洞，进而揭示模型的安全缺陷。理解并应对此类攻击对于推动 LLM 安全领域发展至关重要。以往的越狱方法主要聚焦于对有害意图的直接操纵，却较少关注角色提示（persona prompts）的影响。本研究系统探究了角色提示在突破 LLM 防御机制中的有效性，提出一种基于遗传算法的方法，可自动生成角色提示以绕过 LLM 的安全机制。实验结果表明：（1）经进化生成的角色提示能在多个 LLM 中将拒绝率降低 50%-70%；（2）这些提示与现有攻击方法结合时会产生协同效应，将攻击成功率提升 10%-20%。本研究的代码与数据可在<a target="_blank" rel="noopener" href="https://github.com/CjangCjengh/Generic_Persona%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/CjangCjengh/Generic_Persona获取。</a></p>
<h2 id="本文聚焦的问题">本文聚焦的问题</h2>
<p>1、角色提示是否会影响 LLMs 对越狱攻击的防御能力？</p>
<p>2、若角色提示确实能影响 LLMs 的防御，如何构建此类角色提示以提高 LLMs 对有害请求的依从概率？</p>
<h2 id="本文提出的方法">本文提出的方法</h2>
<p>本文主要通过设计persona prompt来提高越狱攻击成功率。其具体采用了遗传的方式：</p>
<p><img src="https://cdn.jsdelivr.net/gh/i-s-land/Article-pictures/ai/20250826155807880.png" alt=""></p>
<p>首先文章借鉴inCharacter，使用来自小说和电影的N个角色persona描述。因为这些描述通常包含不相关的细节，所以还需要通过LLM来提炼和清理这些描述，从而分离和提炼每个persona的本质。最后生成清理后的persona prompt的集合P<sub>0</sub>。将P<sub>0</sub>传给P<sub>t</sub>。示例如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/i-s-land/Article-pictures/ai/20250826161040189.png" alt=""></p>
<p>然后是交叉与变异。</p>
<p>交叉：在每次迭代中，从当前种群中随机选择M对persona prompt。对于每一对，我们使用一个LLM通过将两个prompt混合在一起来合成一个新的prompt。示例如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/i-s-land/Article-pictures/ai/20250826161146593.png" alt=""></p>
<p>变异：从种群中随机选择M个persona prompt，对于每个选定的prompt，从重写、扩展或收缩中随机选择一种转换。需要注意的是，为了保持prompt长度的平衡，如果一个prompt超过100个单词，文章强制执行收缩，而少于10个单词的prompt则进行扩展。示例如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/i-s-land/Article-pictures/ai/20250826161444713.png" alt=""></p>
<p>再将将当前的种群P<sub>t</sub>与通过交叉和变异生成的新指令P<sub>cross</sub> 和 P<sub>mut</sub> 合并，形成一个更大的集合P<sub>t</sub> ∪ P<sub>cross</sub> ∪ P<sub>mut</sub>。</p>
<p>最后根据LLM对越狱攻击的拒绝率RtA，对P<sub>t</sub> ∪ P<sub>cross</sub> ∪ P<sub>mut</sub>进行排序，排名靠前的N个指令被选中，以形成下一代种群。</p>
<p>这里的分类借助了TrustLLM benchmark提供了一个分类器，其可以用于确定受害者LLM的响应是否包含拒绝，从而计算RtA（拒绝回答）率作为衡量攻击有效性的指标。</p>
<p>整个过程循环往复，直到达到预设的迭代次数或收敛条件。</p>
<h2 id="阅读总结">阅读总结</h2>
<p>优点：</p>
<p>1、自动化性高，效果好。</p>
<p>2、创新研究了persona prompts。</p>
<p>缺点：</p>
<p>1、初始种群依赖现有资源，多样性受限。</p>
<p>2、评估以来大模型，存在潜在主观偏差风险。</p>
<p>未来可以优化初始种群生成机制</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://fdreamer2002.github.io">游俊爽</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://fdreamer2002.github.io/2025/08/26/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-08-30/Enhancing%20Jailbreak%20Attacks%20on%20LLMs%20via%20Persona%20Prompts/">https://fdreamer2002.github.io/2025/08/26/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-08-30/Enhancing%20Jailbreak%20Attacks%20on%20LLMs%20via%20Persona%20Prompts/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://fdreamer2002.github.io" target="_blank">LLM Security Group 's Notes</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95/">遗传算法</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/08/27/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-08-30/WordGame%20Efficient%20&amp;%20Effective%20LLM%20Jailbreak%20via%20Simultaneous%20Obfuscation%20in%20Query%20and%20Response/" title="WordGame: Efficient &amp; Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">WordGame: Efficient & Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response</div></div><div class="info-2"><div class="info-item-1">英文题目：《WordGame: Efficient &amp; Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response》 中文题目：《WordGame：基于查询与响应混淆的大语言模型高效越狱攻击方法》 论文作者：Tianrong Zhang, Bochuan Cao, Yuanpu Cao, Lu Lin, Prasenjit Mitra, Jinghui Chen 发布于： arxiv 发布时间：2024-05-22 级别：无 论文链接：https://doi.org/10.48550/arXiv.2405.14023 论文代码：无  摘要 近期，诸如 ChatGPT 等大型语言模型（LLM）取得的重大突破以前所未有的速度革新了生产流程。与此同时，人们也越来越担忧 LLM 容易遭受破解攻击，从而生成有害或不安全的内容。尽管已经在 LLM 中实施了安全对齐措施来减轻现有的破解尝试，并使其变得越来越复杂，但这些措施仍远非完美。在本文中，我们分析了当前安全对齐的常见模式，并表明可以通过在查询...</div></div></div></a><a class="pagination-related" href="/2025/08/25/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-08-30/PRISM%20Programmatic%20Reasoning%20with%20Image%20Sequence%20Manipulation%20for%20LVLM%20Jailbreaking/" title="PRISM: Programmatic Reasoning with Image Sequence Manipulation for LVLM Jailbreaking"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">PRISM: Programmatic Reasoning with Image Sequence Manipulation for LVLM Jailbreaking</div></div><div class="info-2"><div class="info-item-1">英文题目：《PRISM: Programmatic Reasoning with Image Sequence Manipulation for LVLM Jailbreaking》 中文题目：《PRISM：面向大型视觉语言模型（LVLM）越狱的、基于图像序列操纵的程序化推理》 论文作者：Quanchen Zou, Zonghao Ying, Moyang Chen, Wenzhuo Xu, Yisong Xiao, Yakai Li, Deyue Zhang, Dongdong Yang, Zhao Liu, Xiangzheng Zhang 发布于： arxiv 发布时间：2025-07-29 级别：无 论文链接： https://doi.org/10.48550/arXiv.2507.21540 论文代码：无  摘要 大型视觉语言模型（LVLMs）的复杂程度不断提升，与此同时，旨在防止生成有害内容的安全对齐机制也在逐步发展。然而，这些防御机制在复杂的对抗性攻击面前仍显脆弱。现有越狱方法通常依赖直接且语义明确的提示词，却忽视了大型视觉语言模型（LVLMs）在多步推理过程中整合...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/11/04/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-11-10/AutoDAN%20Generating%20Stealthy%20Jailbreak%20Prompts%20on%20Aligned%20Large%20Language%20Models/" title="AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-04</div><div class="info-item-2">AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models</div></div><div class="info-2"><div class="info-item-1">英文题目：《AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models》 中文题目：《AutoDAN: 在对齐的大型语言模型上生成隐蔽的Jailbreak提示》 论文作者：Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao 发布于： ICLR2024 发布时间：2023-10-03 级别：CCF-A 论文链接：https://doi.org/10.48550/arXiv.2310.04451 论文代码：https://github.com/SheltonLiu-N/AutoDAN  摘要 对齐的大型语言模型(LLMs)是强大的语言理解和决策工具，它们是通过与人类反馈的大量对齐而创建的。然而，这些大型模型仍然容易受到jailbreak攻击，在这种攻击中，攻击者操纵提示以引出不应由对齐的LLM给出的恶意输出。研究jailbreak提示可以使我们深入了解LLM的局限性，并进一步指导我们保护它们。不幸的是，现有的jailbreak技术要么存在(1)...</div></div></div></a><a class="pagination-related" href="/2025/11/10/%E5%8F%A4%E4%B9%89%E7%BF%94/2025-11-10/Open%20Sesame!%20Universal%20Black%20Box%20Jailbreaking%20of%20Large%20Language%20Models/" title="Open Sesame! Universal Black Box Jailbreaking of Large Language Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-10</div><div class="info-item-2">Open Sesame! Universal Black Box Jailbreaking of Large Language Models</div></div><div class="info-2"><div class="info-item-1">英文题目：《Open Sesame! Universal Black Box Jailbreaking of Large Language Models》 中文题目：《芝麻开门！大型语言模型的通用黑盒越狱》 论文作者：Raz Lapid, Ron Langberg, Moshe Sipper 发布于：CoRR 2023 论文链接：http://arxiv.org/abs/2309.01446 代码链接：无   摘要 该工作提出并验证了一种基于遗传算法（GA）的通用黑盒越狱框架：在目标 LLM 完全黑盒（仅能查询输出）的条件下，进化出一段固定的 token 后缀，将其拼接到任意用户输入后能显著提高模型输出与“服从/有害回答”语义的相似度，从而实现普适性的越狱效果。与白盒梯度方法不同，本方法不依赖模型内部信息，具有跨输入与（一定程度的）跨模型迁移性，同时通过随机子集评估等手段降低查询成本。 本文聚焦的问题 如何在黑盒中实现自动化的黑盒越狱对抗性后缀。 本文提出的方法 1. 方法目标  目标：寻找一个通用的对抗后缀 x_adv，对于数据集中大部分有害指令 x_user，拼接后 x = ...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">LLM Security Group</div><div class="author-info-description">分享知识，认识世界</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">92</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">92</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">27</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E8%81%9A%E7%84%A6%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">2.</span> <span class="toc-text">本文聚焦的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E6%8F%90%E5%87%BA%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">本文提出的方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%98%85%E8%AF%BB%E6%80%BB%E7%BB%93"><span class="toc-number">4.</span> <span class="toc-text">阅读总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/10/%E5%8F%A4%E4%B9%89%E7%BF%94/2025-11-10/A%20Wolf%20in%20Sheep%E2%80%99s%20Clothing%20Generalized%20Nested%20Jailbreak%20Prompts%20can%20Fool%20Large%20Language%20Models%20Easily/" title="A Wolf in Sheep’s Clothing Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily">A Wolf in Sheep’s Clothing Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily</a><time datetime="2025-11-09T16:00:00.000Z" title="发表于 2025-11-10 00:00:00">2025-11-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/10/%E5%8F%A4%E4%B9%89%E7%BF%94/2025-11-10/Open%20Sesame!%20Universal%20Black%20Box%20Jailbreaking%20of%20Large%20Language%20Models/" title="Open Sesame! Universal Black Box Jailbreaking of Large Language Models">Open Sesame! Universal Black Box Jailbreaking of Large Language Models</a><time datetime="2025-11-09T16:00:00.000Z" title="发表于 2025-11-10 00:00:00">2025-11-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/04/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-11-10/PAPILLON%20Efficient%20and%20Stealthy%20Fuzz%20Testing-Powered%20Jailbreaks%20for%20LLMs/" title="PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs">PAPILLON: Efficient and Stealthy Fuzz Testing-Powered Jailbreaks for LLMs</a><time datetime="2025-11-04T11:58:14.000Z" title="发表于 2025-11-04 19:58:14">2025-11-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/04/%E4%BC%8D%E4%BF%8A/2025-11-07/SNIS%20A%20Signal%20Noise%20Separation-Based%20Network%20%20for%20Post-Processed%20Image%20Forgery%20Detection/" title="SNIS: A Signal Noise Separation-Based Network  for Post-Processed Image Forgery Detection">SNIS: A Signal Noise Separation-Based Network  for Post-Processed Image Forgery Detection</a><time datetime="2025-11-04T11:53:16.000Z" title="发表于 2025-11-04 19:53:16">2025-11-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/04/%E6%B8%B8%E4%BF%8A%E7%88%BD/2025-11-10/AutoDAN%20Generating%20Stealthy%20Jailbreak%20Prompts%20on%20Aligned%20Large%20Language%20Models/" title="AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models">AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models</a><time datetime="2025-11-04T07:51:14.000Z" title="发表于 2025-11-04 15:51:14">2025-11-04</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By LLM Security Group</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.3</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script src="https://cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/DynamicRibbon.min.js"></script><script src="https://cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/star.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="输入关键词…" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>